# 2. Image Classification

## Computer Vision and Image Classification

**ì»´í“¨í„° ë¹„ì „(Computer Vision)**
 - ê¸°ê³„ì˜ ì‹œê°ì— í•´ë‹¹í•˜ëŠ” ë¶€ë¶„ì„ ì—°êµ¬í•˜ëŠ” ë¶„ì•¼
 - ê³µí•™ì ì¸ ê´€ì ì—ì„œ, ì¸ê°„ì˜ ì‹œê°ì´ í•  ìˆ˜ ìˆëŠ” ëª‡ê°€ì§€ ì¼ì„ ìˆ˜í–‰í•˜ëŠ” ììœ¨ì ì¸ ì‹œìŠ¤í…œì„ ë§Œë“œëŠ” ê²ƒì„ ëª©í‘œ

 ë¶„ì•¼: ì´ë¯¸ì§€ ë¶„ë¥˜, ìœ„ì¹˜ ì¸ì‹, ë¬¼ì²´ ê²€ì¶œ, ì´ë¯¸ì§€ ìº¡ì…”ë‹

**ì´ë¯¸ì§€ ë¶„ë¥˜(Image Classification)**
![Image Classification](https://user-images.githubusercontent.com/59776953/113523115-de511200-95e0-11eb-98cd-cef16b96d2de.png)
ì´ë¯¸ì§€ ì „ì²´ í˜¹ì€ ì´ë¯¸ì§€ ì•ˆì˜ ë¬¼ì²´(object)ì˜ ì¢…ë¥˜ë¥¼ êµ¬ë¶„í•˜ëŠ” ì‘ì—…

**ë¬¼ì²´ ìœ„ì¹˜ì¸ì‹(Object Localization)**
![localization](https://user-images.githubusercontent.com/59776953/113523118-e27d2f80-95e0-11eb-856a-53cffb9b7c49.png)

ì´ë¯¸ì§€ ì•ˆì˜ ë¬¼ì²´ê°€ ì´ë¯¸ì§€ì˜ ì–´ëŠ ì˜ì—­ì— ìˆëŠ”ì§€ ìœ„ì¹˜ ì •ë³´ë¥¼ ì¶œë ¥í•´ì£¼ëŠ” ì‘ì—…

**ë¬¼ì²´ ê²€ì¶œ(Object Detection)**
![Object Detection](https://user-images.githubusercontent.com/59776953/113523127-eb6e0100-95e0-11eb-8f33-693db307764f.png)
ë¬¼ì²´ê°€ ë¬´ì—‡ì¸ì§€ ë¶„ë¥˜(classification)í•˜ëŠ” ê³¼ì •ê³¼ ë¬¼ì²´ê°€ ì–´ë””ì— ìˆëŠ”ì§€ ìœ„ì¹˜ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ëŠ” ìœ„ì¹˜ì¸ì‹(localization) ê³¼ì •ì´ ë™ì‹œì— ìˆ˜í–‰ë˜ëŠ” ì‘ì—…

**ì´ë¯¸ì§€ ìº¡ì…”ë‹(Image Captioning)**
 ![Image Captioning](https://user-images.githubusercontent.com/59776953/113523113-dd1fe500-95e0-11eb-8461-2120cea7e0d1.png)
 ì´ë¯¸ì§€ë¥¼ ì„¤ëª…í•˜ëŠ” ë¬¸ì¥ì„ ë§Œë“¤ì–´ë‚´ëŠ” ì‘ì—…
 
## Challenges of Image Classification

![problem](https://user-images.githubusercontent.com/59776953/113523170-28d28e80-95e1-11eb-9554-d43c281698ae.png)
**"ìˆ«ìë¡œ êµ¬ì„±ëœ 3D Array(height x width x color channel)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì–´ë–»ê²Œ ì´ë¯¸ì§€ ë¶„ë¥˜ë¥¼ í• ê¹Œ?"**

Challenges
1. Viewpoint Variation - ì‹œê°ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ë³´ì¸ë‹¤
2. Illumination - ì‚¬ì§„ê³¼ ë°°ê²½ì˜ ë°ê¸°
3. Deformation - í˜•íƒœì˜ ë³€í˜•
4. Occulusion - ê°€ë ¤ì ¸ ìˆê±°ë‚˜ ì¼ë¶€ë§Œ ë³´ì¼ ë•Œ
5. Background Clutter - ë°°ê²½ê³¼ êµ¬ë¶„ì´ ì˜ ì•ˆë  ë•Œ
6. Interclass Variation - ë‹¤ì–‘í•œ ì¢…ì„ ëª¨ë‘ íŒë‹¨í•´ì•¼ í•¨

## Rule-Based Approach vs. Data-Driven Approach

**Rule-Based Approach**

ì´ˆê¸°ì—ëŠ”, íŠ¹ì •í•œ edge, shape, junctionì„ ì°¾ì•„ íŠ¹ì • ì•Œê³ ë¦¬ì¦˜ì„ í†µí•˜ì—¬ íŒë‹¨í•˜ëŠ” Rule-Based Approachë¥¼ ì‚¬ìš©í•˜ì˜€ë‹¤. 

ë¬¸ì œì 
1. ê·œì¹™ì„ ë§Œë“¤ì–´ë‚´ê¸° ì–´ë ¤ì›€
2. í™•ì¥ì„±(Scalability)ê°€ ë–¨ì–´ì§

![rule_vs_data](https://user-images.githubusercontent.com/59776953/113523350-579d3480-95e2-11eb-9a4c-94e16923d7f1.png)

**Data-Driven Approach**

ì‚¬ëŒì´ ì§ì ‘ ì•Œê³ ë¦¬ì¦˜ì„ ë§Œë“œëŠ”ê²Œ ì•„ë‹ˆë¼, ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ì„ ë§Œë“¤ì–´ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë°©ë²•

1. ë¼ë²¨í™”ëœ ì´ë¯¸ì§€ë¥¼ ìˆ˜ì§‘í•œë‹¤
2. ë¨¸ì‹ ëŸ¬ë‹ì„ ì´ìš©í•˜ì—¬ Classifierì„ trainí•œë‹¤
3. ìƒˆë¡œìš´ ì´ë¯¸ì§€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ Classifierì˜ ì„±ëŠ¥ì„ í‰ê°€í•œë‹¤

## Nearest Neighbor

**Nearest Neighbor Search**
![Nearest Neighbor](https://user-images.githubusercontent.com/59776953/113523304-042ae680-95e2-11eb-9f76-c3823b7d0dc6.png)
ê°€ì¥ ê°€ê¹Œìš´ ì ì„ ì°¾ê¸° ìœ„í•œ ìµœì í™” ë¬¸ì œ

ê±°ë¦¬ ê³„ì‚° ë°©ì‹
**L1 Distance(Manhattan Distance)**
![l1 distance](https://user-images.githubusercontent.com/59776953/113523229-7e0ea000-95e1-11eb-9196-7e00b47d63ce.png)
ë‘ ê°œì˜ ë²¡í„°ë¥¼ ë¹¼ê³  ì ˆëŒ€ê°’ì„ ì·¨í•œ ë’¤ í•©í•˜ëŠ” ë°©ì‹

![nearest neighbor code](https://user-images.githubusercontent.com/59776953/113523216-6b946680-95e1-11eb-94ff-eeb6b0d762bd.png)
```python
import numpy as np class
NearestNeighbor(object):  
	def __init__(self):  
		pass  
	def train(self, X, y):  
		""" X is N x D where each row is an example.Y is 1-dimension of size N """  
		# the nearest neighbor classifier simply remembers all the training data 
		self.Xtr = X 
		self.ytr = y 
	def predict(self, X):  
		""" X is N x D where each row is an example we wish to predict label for """ 
		num_test = X.shape[0]  
		# lets make sure that the output type matches the input type 
		Ypred = np.zeros(num_test, dtype = self.ytr.dtype)  

		# loop over all test rows  
		for i in  xrange(num_test):  
			# find the nearest training image to the i'th test image  
			# using the L1 distance (sum of absolute value differences) 
			distances = np.sum(np.abs(self.Xtr - X[i,:]), axis =  1) 
			min_index = np.argmin(distances)  # get the index with smallest distance 
			Ypred[i]  = self.ytr[min_index]  # predict the label of the nearest example  

		return Ypred  
```

ë¬¸ì œì 
- Nearest Neighborì˜ ê²½ìš°, ë©”ëª¨ë¦¬ ìƒì— training dataì™€ labelì„ ëª¨ë‘ ì˜¬ë¦¬ê³  test ë°ì´í„°ì™€ì˜ distanceë¥¼ ì‚´í•Œ
- training dataê°€ 2ë°° ëŠ˜ì–´ë‚˜ë©´ ë¶„ë¥˜ ì‘ì—… ì‹œê°„ë„ 2ë°° ëŠ˜ì–´ë‚˜ê²Œ ë¨ ğŸ – test ì‹œ, ì˜¤ë˜ ê±¸ë¦¼
- CNNì—ì„œëŠ” trainingì—ëŠ” ì˜¤ë˜ ê±¸ë¦¬ì§€ë§Œ, test ì‹œ êµ‰ì¥íˆ ì§§ê²Œ ê±¸ë¦¼

## k-Nearest Neighbors

**k-NN Classification**
 íŠ¹ì§• ê³µê°„ ë‚´ kê°œì˜ ê°€ì¥ ê°€ê¹Œìš´ í›ˆë ¨ ë°ì´í„° ì‚¬ì´ì—ì„œ ê°€ì¥ ê³µí†µì ì¸ í•­ëª©ì— í• ë‹¹

**Hyperparameter**

ê±°ë¦¬ ê³„ì‚° ë°©ì‹
![manhattan_euclidean](https://user-images.githubusercontent.com/59776953/113523121-e4df8980-95e0-11eb-988c-fddcb372ee76.png)
**L1 Distance(Manhattan Distance)**
ë‘ ê°œì˜ ë²¡í„°ë¥¼ ë¹¼ê³  ì ˆëŒ€ê°’ì„ ì·¨í•œ ë’¤ í•©í•˜ëŠ” ë°©ì‹

**L2 Distance(Euclidean Distance)**
ë‘ ê°œ ë²¡í„° ì‚¬ì´ì˜ ì§ì„  ê±°ë¦¬

kê°’ 
![k](https://user-images.githubusercontent.com/59776953/113523427-86b3a600-95e2-11eb-8711-2b6b64b96ac7.png)

ìœ„ì˜ ê·¸ë¦¼ì—ì„œ kNNì´ NNë³´ë‹¤ ì´ìƒì¹˜ì— ë‘”ê°í•œ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. kê°€ ë†’ì•„ì§€ë©´ ê·¸ë§Œí¼ ê³„ì‚°ëŸ‰ì´ ë§ì•„ì§€ëŠ” ë‹¨ì ì´ ìˆë‹¤.

- ì ì ˆí•œ kê°’ ì •í•˜ê¸°
- L1 Distance/L2 Distance ë°©ì‹ ì¤‘ ë” ë‚˜ì€ ë°©ì‹ ì„ íƒí•˜ê¸°

## Hyperparameter ì„¤ì •

**Hyperparameter ì„¤ì •ì„ ìœ„í•´ Validation Data ë§ˆë ¨**

**êµì°¨ ê²€ì¦(Cross Validation)**
![k-fold cross validation](https://user-images.githubusercontent.com/59776953/113523090-bcf02600-95e0-11eb-8cba-e96d1b4a2716.png)

ê³ ì •ëœ training setìœ¼ë¡œ ëª¨ë¸ì„ ë§Œë“¤ ê²½ìš° ê³¼ì í•©ì´ ì¼ì–´ë‚  ìˆ˜ ìˆëŠ”ë° ì´ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ êµì°¨ ê²€ì¦ì„ ì‚¬ìš©í•œë‹¤

K-Fold Cross Validation ê³¼ì •
1. ì „ì²´ ë°ì´í„°ì…‹ì„ training setê³¼ test setìœ¼ë¡œ ë‚˜ëˆˆë‹¤
2. training setì„ training set + validation setìœ¼ë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ kê°œì˜ foldë¡œ ë‚˜ëˆˆë‹¤
3.  ì²« ë²ˆì§¸ foldë¥¼ validation setìœ¼ë¡œ ì‚¬ìš©í•˜ê³  ë‚˜ë¨¸ì§€ foldë“¤ì„ training setìœ¼ë¡œ ì‚¬ìš©í•œë‹¤
4. ëª¨ë¸ì„ trainingí•œ ë’¤, ì²« ë²ˆì§¸ validation setìœ¼ë¡œ í‰ê°€í•œë‹¤
5. ì°¨ë¡€ëŒ€ë¡œ ë‹¤ìŒ foldë¥¼ validation setìœ¼ë¡œ ì‚¬ìš©í•˜ë©° 3ì„ ë°˜ë³µí•œë‹¤
6. ì´ k ê°œì˜ ì„±ëŠ¥ ê²°ê³¼ê°€ ë‚˜ì˜¤ë©°, ì´ k ê°œì˜ í‰ê· ì„ í•´ë‹¹ í•™ìŠµ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ë¼ê³  í•œë‹¤

## Linear Classification

 **ì„ í˜• ë¶„ë¥˜(Linear Classification)**
 ì„ ì„ ì´ìš©í•˜ì—¬ ì…ë ¥ê°’ì— ëŒ€í•´ ì—¬ëŸ¬ class ì¤‘ í•˜ë‚˜ë¥¼ íƒí•´ ë¶„ë¥˜í•˜ëŠ” ëª¨ë¸
 - parameter ê¸°ë°˜ì˜ ì ‘ê·¼ ë°©ì‹
 (NNì€ nonparametric approach)

![linear classifier](https://user-images.githubusercontent.com/59776953/113523266-c1690e80-95e1-11eb-8ef2-5111cf434726.png)

![linear classifier2](https://user-images.githubusercontent.com/59776953/113523267-c332d200-95e1-11eb-9c79-c925a4c7886d.png)

Classifierê°€ ë‚´ë†“ì€ ê²°ê³¼ ê°’ì— ëŒ€í•´ ì œëŒ€ë¡œ ë¶„ë¥˜í–ˆë‚˜ í‰ê°€í•˜ê¸° ìœ„í•˜ì—¬ ì •ë‹µ labelê³¼ ë¹„êµë¥¼ í•˜ëŠ”ë° ì´ ë•Œ ë¹„êµí•˜ëŠ” í•¨ìˆ˜ê°€ **loss function**

- Y = W x + bias ì¢Œí‘œ ìƒ í•œ ì§ì„ ìœ¼ë¡œ ë‚˜ëˆ„ì§€ ëª»í•˜ë©´ linear classifierê°€ í†µí•˜ì§€ ì•ŠìŒ

ì¶œì²˜
- https://m.blog.naver.com/arar2017/221791751470
- https://3months.tistory.com/512
- https://ko.wikipedia.org/wiki/K-%EC%B5%9C%EA%B7%BC%EC%A0%91_%EC%9D%B4%EC%9B%83_%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98
- https://ko.wikipedia.org/wiki/%EC%B5%9C%EA%B7%BC%EC%A0%91_%EC%9D%B4%EC%9B%83_%ED%83%90%EC%83%89
